
# ZIP File Downloader
# ----------------------
#
# This script automates the downloading of ZIP files from a list of URLs.
# It requires a JSON file containing the visited URLs generated by `web_automation_url_analysis.py`.


import asyncio
import json
from pyppeteer import launch
from pyppeteer.errors import TimeoutError




async def click_download_zip(url):
    """
    Automate the clicking of the ZIP file download button on the provided URL.
    
    Parameters:
    - url (str): The URL to click the download button on.
    """
    browser = await launch(headless=False)
    page = await browser.newPage()

    await page.goto(url)
    await page._client.send('Network.enable')
    await page._client.send('Page.enable')
    await page._client.send('Runtime.enable')
    await page._client.send('DOM.enable')
    await page._client.send('CSS.enable')
    await page._client.send('Overlay.enable')
    await page._client.send('Log.enable')

    try:
        b_green = await page.querySelectorAll('.Button-content')
        for element in b_green:
            await element.click()
    except Exception as exc:
        print('Please provide a correct locator!')
        await browser.close()

    try:
        b_zip_download = await page.waitForSelector('a.d-flex.flex-items-center.color-fg-default.text-bold.no-underline[href$=".zip"]', {'timeout': 5 * 1000}) # 5 sec.
        await b_zip_download.click()

        # Wait for the ZIP file to download
        # await page.waitForNavigation(waitUntil='networkidle0')
        # await browser.close()

    except TimeoutError:
        print('Timeout occurred because the correct locator was not found!')
    except Exception as exc:
        print('Please provide a correct locator!', exc)
        await browser.close()


async def main():
    """
    Main function for executing the ZIP file downloading tasks.
    Reads the visited URLs from a JSON file and initiates the download tasks.
    """
    with open('data/visited_urls_2023-06-16.json') as f:
        data = json.load(f)
        urls = [n.get('url') for n in data]

        tasks = [click_download_zip(url) for url in urls]
        await asyncio.gather(*tasks)


await main()

